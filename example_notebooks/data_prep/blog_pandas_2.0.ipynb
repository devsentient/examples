{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb51063-384f-49be-8c2a-3465e9695ff3",
   "metadata": {},
   "source": [
    "# Pandas 2.0: Guide to Upgrading and Adapting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36ee81-7a35-4828-8b4d-4fcd306f75fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721f3d1e-dc32-4fe3-a82b-825ce0f5e473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82e4f4-6929-4479-8ff1-2414cf84b819",
   "metadata": {},
   "source": [
    "## Improved Nullable Dtypes and Extension Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53581cf-8c7b-47ef-8987-5fa620beed56",
   "metadata": {},
   "source": [
    "Improved Nullable Dtypes and Extension Arrays\n",
    "Pandas 2.0 brings faster and more memory-efficient operations to the table by adding support for PyArrow in the backend. \n",
    "\n",
    "This code demonstrates reading a CSV file with sample data, converting numeric columns to nullable data types, and saving and reading the data as a Parquet file using the pyarrow engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93656e7-0910-4e09-aec9-c984f544af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# New sample data\n",
    "new_sample_data = io.StringIO(\"\"\"Category,Value,Flag,Label,Count,Rating,Percentage,Status,Code\n",
    "    Fruit,100,True,Apple,25,4.5,0.50,InStock,A1\n",
    "    Vegetable,200,False,Carrot,30,3.8,0.35,OutOfStock,B2\n",
    "    Grain,150,True,Rice,20,4.2,0.25,InStock,C3\n",
    "\"\"\")\n",
    "\n",
    "# Reading CSV with pandas-backed nullable dtypes\n",
    "data_frame = pd.read_csv(new_sample_data)\n",
    "\n",
    "# Converting numeric columns to nullable dtypes\n",
    "data_frame = data_frame.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "data_frame.to_parquet(\"data_frame.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "data_frame_from_parquet = pd.read_parquet(\"data_frame.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b595b4c7-8fdd-4f67-8b62-b8d25b17e6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (3, 9)\n",
      "Columns in the DataFrame: ['Category', 'Value', 'Flag', 'Label', 'Count', 'Rating', 'Percentage', 'Status', 'Code']\n",
      "\n",
      "Summary statistics of the DataFrame:\n",
      "       Value  Count    Rating  Percentage\n",
      "count    3.0    3.0  3.000000    3.000000\n",
      "mean   150.0   25.0  4.166667    0.366667\n",
      "std     50.0    5.0  0.351188    0.125831\n",
      "min    100.0   20.0  3.800000    0.250000\n",
      "25%    125.0   22.5  4.000000    0.300000\n",
      "50%    150.0   25.0  4.200000    0.350000\n",
      "75%    175.0   27.5  4.350000    0.425000\n",
      "max    200.0   30.0  4.500000    0.500000\n",
      "\n",
      "Unique values in the 'Category' column: ['    Fruit' '    Vegetable' '    Grain']\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {data_frame.shape}\")\n",
    "\n",
    "# Print the columns in the DataFrame\n",
    "print(f\"Columns in the DataFrame: {data_frame.columns.tolist()}\")\n",
    "\n",
    "# Print summary statistics of the DataFrame\n",
    "print(\"\\nSummary statistics of the DataFrame:\")\n",
    "print(data_frame.describe())\n",
    "\n",
    "# Print unique values in the 'Category' column\n",
    "print(f\"\\nUnique values in the 'Category' column: {data_frame['Category'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30829fc-a102-4d1c-b19d-3689ae21c789",
   "metadata": {},
   "source": [
    "## Copy-on-Write (CoW) Improvements "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46f6ed-ddee-409a-ba4a-64eff35189f6",
   "metadata": {},
   "source": [
    "By enabling CoW, Pandas can avoid making defensive copies when performing various operations, and instead, it only makes copies when necessary, which results in more efficient memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c43c93a7-9244-4cbd-b248-0a49ba0256b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a    b  c\n",
      "0  1  4.0  x\n",
      "1  2  5.0  y\n",
      "2  3  6.0  z\n",
      "   a    b  c\n",
      "0  7  4.0  x\n",
      "1  8  5.0  y\n",
      "2  9  6.0  z\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "data = {\"a\": [1, 2, 3], \"b\": [4.0, 5.0, 6.0], \"c\": [\"x\", \"y\", \"z\"]}\n",
    "df1 = pd.DataFrame(data)\n",
    "df2 = df1.copy()\n",
    "\n",
    "df2[\"a\"] = [7, 8, 9]\n",
    "\n",
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29900325-49cd-47ed-9f38-77cf8d45a1e1",
   "metadata": {},
   "source": [
    "## Handling Differences in Data Type Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e59e8-7b68-4513-9a77-5b0974e2e8ed",
   "metadata": {},
   "source": [
    "Hereâ€™s an illustration of the process of creating a Pandas DataFrame that incorporates Apache Arrow-backed data types within a practical context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ea4fe6-16c1-46b1-a1b2-bf25a51f7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         product_name                                        features   \n",
      "0  Samsung Galaxy S22            [5G, AMOLED display, 128 GB storage]  \\\n",
      "1   iPhone 15 Pro Max  [5G, Super Retina XDR display, 512 GB storage]   \n",
      "\n",
      "  release_date  \n",
      "0   2022-12-10  \n",
      "1   2022-09-30  \n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Example of using new Arrow-based data types in Pandas 2.0\n",
    "data = pd.DataFrame({\n",
    "    'product_name': pd.Series(['Samsung Galaxy S22',\n",
    "                               'iPhone 15 Pro Max'],\n",
    "                              dtype='string'),\n",
    "    'features': pd.Series([['5G', 'AMOLED display', '128 GB storage'],\n",
    "                           ['5G', 'Super Retina XDR display', '512 GB storage']],\n",
    "                          dtype='object'),\n",
    "    'release_date': pd.Series([datetime.date(2022, 12, 10),\n",
    "                               datetime.date(2022, 9, 30)],\n",
    "                              dtype='datetime64[ns]')\n",
    "})\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397e781-cce5-47b7-8e45-844e119641ab",
   "metadata": {},
   "source": [
    "## Evaluating Performance Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd8948-e108-441a-931f-312e21e42e2d",
   "metadata": {},
   "source": [
    "In many cases, the performance will be significantly improved with Pandas 2.0, especially when working with large datasets. However, some operations might be slower or not yet optimized, so it's crucial to benchmark your code and compare the performance with other tools or previous versions of Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e0bab-307d-4b30-b820-13cda03e67d3",
   "metadata": {},
   "source": [
    "Here's an example of how you can measure the performance of different data manipulation tasks in Pandas 2.0 compared to other data processing libraries such as Polars, DuckDB, and Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c57b57-8a4b-4d5d-9c48-a3f466a11c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a06ce-2707-4169-9243-bab7049aca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b1b06a-3eb1-41dd-aef0-ddaa45fa761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install dask --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202e4ec-d9d5-4831-aa1a-a3243ba1af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pandas polars duckdb dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f373d7-c875-48f1-83f5-80e21be59bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas 2.0: 1.19203 seconds\n",
      "Polars: 1.14821 seconds\n",
      "DuckDB: 11.92281 seconds\n",
      "Dask: 1.40523 seconds\n",
      "CPU times: user 22.5 s, sys: 2.73 s, total: 25.2 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import duckdb\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Prepare data\n",
    "data = pd.DataFrame({\n",
    "    'A': list(range(1000000)),\n",
    "    'B': list(range(1000000, 2000000))\n",
    "})\n",
    "\n",
    "# Pandas 2.0\n",
    "def pandas_operation():\n",
    "    return data.groupby('A').sum()\n",
    "\n",
    "pandas_time = timeit.timeit(pandas_operation, number=10)\n",
    "\n",
    "# Polars\n",
    "polars_data = pl.from_pandas(data)\n",
    "\n",
    "def polars_operation():\n",
    "    return polars_data.groupby('A').agg(pl.col('B').sum())\n",
    "\n",
    "polars_time = timeit.timeit(polars_operation, number=10)\n",
    "\n",
    "# DuckDB\n",
    "duckdb_conn = duckdb.connect(database=':memory:', read_only=False)\n",
    "duckdb_conn.register('data', data)\n",
    "duckdb_cursor = duckdb_conn.cursor()\n",
    "\n",
    "def duckdb_operation():\n",
    "    duckdb_cursor.execute('SELECT A, SUM(B) FROM data GROUP BY A')\n",
    "    return duckdb_cursor.fetchall()\n",
    "\n",
    "duckdb_time = timeit.timeit(duckdb_operation, number=10)\n",
    "\n",
    "# Dask\n",
    "dask_data = dd.from_pandas(data, npartitions=4)\n",
    "\n",
    "def dask_operation():\n",
    "    return dask_data.groupby('A').sum().compute()\n",
    "\n",
    "dask_time = timeit.timeit(dask_operation, number=10)\n",
    "\n",
    "# Print results\n",
    "print(f\"Pandas 2.0: {pandas_time:.5f} seconds\")\n",
    "print(f\"Polars: {polars_time:.5f} seconds\")\n",
    "print(f\"DuckDB: {duckdb_time:.5f} seconds\")\n",
    "print(f\"Dask: {dask_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495424f-ce31-40a5-b0f3-c38c129a75c4",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85671ae6-5a3f-41e0-b76a-f27fcef1eb61",
   "metadata": {},
   "source": [
    "Pandas 2.0 represents a significant milestone for the library, as the integration of Apache Arrow allows for simpler, faster, and more efficient data processing tasks.\n",
    "For more information, you can also consult the official release notes and GitHub repository of Pandas 2.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
