{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548d2f3c",
   "metadata": {},
   "source": [
    "## (Advanced) PopulationBasedTraining with Ray Tune, TensorFlow, MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a2f7d",
   "metadata": {},
   "source": [
    "## step 1 import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d9a6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray version 1.8.0\n",
      "xgboost_ray 0.1.5\n",
      "xgboost 1.5.1\n",
      "lightgbm_ray 0.1.2\n",
      "pandas version 1.2.3\n"
     ]
    }
   ],
   "source": [
    "import ray; print(f'ray version {ray.__version__}')\n",
    "import xgboost_ray; print('xgboost_ray', xgboost_ray.__version__)\n",
    "import xgboost; print('xgboost', xgboost.__version__)\n",
    "import lightgbm_ray; print('lightgbm_ray', lightgbm_ray.__version__)\n",
    "import pandas as pd; print('pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26a7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import (Input, Activation, Dense, Permute,\n",
    "                                     Dropout)\n",
    "from tensorflow.keras.layers import add, dot, concatenate\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from filelock import FileLock\n",
    "import os\n",
    "import argparse\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from ray import tune\n",
    "import mlflow\n",
    "from ray.tune.integration.mlflow import MLflowLoggerCallback, mlflow_mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1324566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    \"\"\"Return the tokens of a sentence including punctuation.\n",
    "\n",
    "    >>> tokenize(\"Bob dropped the apple. Where is the apple?\")\n",
    "    [\"Bob\", \"dropped\", \"the\", \"apple\", \".\", \"Where\", \"is\", \"the\", \"apple\", \"?\"]\n",
    "    \"\"\"\n",
    "    return [x.strip() for x in re.split(r\"(\\W+)?\", sent) if x and x.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a945b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stories(lines, only_supporting=False):\n",
    "    \"\"\"Parse stories provided in the bAbi tasks format\n",
    "\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode(\"utf-8\").strip()\n",
    "        nid, line = line.split(\" \", 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if \"\\t\" in line:\n",
    "            q, a, supporting = line.split(\"\\t\")\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append(\"\")\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a7c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    \"\"\"Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    \"\"\"\n",
    "\n",
    "    def flatten(data):\n",
    "        return sum(data, [])\n",
    "\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d528d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(word_idx, story_maxlen, query_maxlen, data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen), np.array(answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfa38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(finish_fast=False):\n",
    "    # Get the file\n",
    "    try:\n",
    "        path = get_file(\n",
    "            \"babi-tasks-v1-2.tar.gz\",\n",
    "            origin=\"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\")\n",
    "            # origin = \"gs://shakdemo-hyperplane/data/nlp/babi_tasks_1-20_v1-2.tar.gz\")\n",
    "            # origin = \"s3://d2v-tmp/demo/data/qa/babi_tasks_1-20_v1-2.tar.gz\")\n",
    "    except Exception:\n",
    "        print(\n",
    "            \"Error downloading dataset, please download it manually:\\n\"\n",
    "            \"$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2\"  # noqa: E501\n",
    "            \".tar.gz\\n\"\n",
    "            \"$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz\"  # noqa: E501\n",
    "        )\n",
    "        raise\n",
    "\n",
    "    # Choose challenge\n",
    "    challenges = {\n",
    "        # QA1 with 10,000 samples\n",
    "        \"single_supporting_fact_10k\": \"tasks_1-20_v1-2/en-10k/qa1_\"\n",
    "        \"single-supporting-fact_{}.txt\",\n",
    "        # QA2 with 10,000 samples\n",
    "        \"two_supporting_facts_10k\": \"tasks_1-20_v1-2/en-10k/qa2_\"\n",
    "        \"two-supporting-facts_{}.txt\",\n",
    "    }\n",
    "    challenge_type = \"single_supporting_fact_10k\"\n",
    "    challenge = challenges[challenge_type]\n",
    "\n",
    "    with tarfile.open(path) as tar:\n",
    "        train_stories = get_stories(tar.extractfile(challenge.format(\"train\")))\n",
    "        test_stories = get_stories(tar.extractfile(challenge.format(\"test\")))\n",
    "    if finish_fast:\n",
    "        train_stories = train_stories[:64]\n",
    "        test_stories = test_stories[:64]\n",
    "    return train_stories, test_stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fb11fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemNNModel(tune.Trainable):\n",
    "    @mlflow_mixin\n",
    "    def build_model(self):\n",
    "        \"\"\"Helper method for creating the model\"\"\"\n",
    "        vocab = set()\n",
    "        for story, q, answer in self.train_stories + self.test_stories:\n",
    "            vocab |= set(story + q + [answer])\n",
    "        vocab = sorted(vocab)\n",
    "\n",
    "        # Reserve 0 for masking via pad_sequences\n",
    "        vocab_size = len(vocab) + 1\n",
    "        story_maxlen = max(\n",
    "            len(x) for x, _, _ in self.train_stories + self.test_stories)\n",
    "        query_maxlen = max(\n",
    "            len(x) for _, x, _ in self.train_stories + self.test_stories)\n",
    "\n",
    "        word_idx = {c: i + 1 for i, c in enumerate(vocab)}\n",
    "        self.inputs_train, self.queries_train, self.answers_train = (\n",
    "            vectorize_stories(word_idx, story_maxlen, query_maxlen,\n",
    "                              self.train_stories))\n",
    "        self.inputs_test, self.queries_test, self.answers_test = (\n",
    "            vectorize_stories(word_idx, story_maxlen, query_maxlen,\n",
    "                              self.test_stories))\n",
    "\n",
    "        # placeholders\n",
    "        input_sequence = Input((story_maxlen, ))\n",
    "        question = Input((query_maxlen, ))\n",
    "\n",
    "        # encoders\n",
    "        # embed the input sequence into a sequence of vectors\n",
    "        input_encoder_m = Sequential()\n",
    "        input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=64))\n",
    "        input_encoder_m.add(Dropout(self.config.get(\"dropout\", 0.3)))\n",
    "        # output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "        # embed the input into a sequence of vectors of size query_maxlen\n",
    "        input_encoder_c = Sequential()\n",
    "        input_encoder_c.add(\n",
    "            Embedding(input_dim=vocab_size, output_dim=query_maxlen))\n",
    "        input_encoder_c.add(Dropout(self.config.get(\"dropout\", 0.3)))\n",
    "        # output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "        # embed the question into a sequence of vectors\n",
    "        question_encoder = Sequential()\n",
    "        question_encoder.add(\n",
    "            Embedding(\n",
    "                input_dim=vocab_size, output_dim=64,\n",
    "                input_length=query_maxlen))\n",
    "        question_encoder.add(Dropout(self.config.get(\"dropout\", 0.3)))\n",
    "        # output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "        # encode input sequence and questions (which are indices)\n",
    "        # to sequences of dense vectors\n",
    "        input_encoded_m = input_encoder_m(input_sequence)\n",
    "        input_encoded_c = input_encoder_c(input_sequence)\n",
    "        question_encoded = question_encoder(question)\n",
    "\n",
    "        # compute a \"match\" between the first input vector sequence\n",
    "        # and the question vector sequence\n",
    "        # shape: `(samples, story_maxlen, query_maxlen)`\n",
    "        match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "        match = Activation(\"softmax\")(match)\n",
    "\n",
    "        # add the match matrix with the second input vector sequence\n",
    "        response = add(\n",
    "            [match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "        response = Permute(\n",
    "            (2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "        # concatenate the match matrix with the question vector sequence\n",
    "        answer = concatenate([response, question_encoded])\n",
    "\n",
    "        # the original paper uses a matrix multiplication.\n",
    "        # we choose to use a RNN instead.\n",
    "        answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "        # one regularization layer -- more would probably be needed.\n",
    "        answer = Dropout(self.config.get(\"dropout\", 0.3))(answer)\n",
    "        answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "        # we output a probability distribution over the vocabulary\n",
    "        answer = Activation(\"softmax\")(answer)\n",
    "\n",
    "        # build the final model\n",
    "        model = Model([input_sequence, question], answer)\n",
    "        return model\n",
    "    \n",
    "    @mlflow_mixin\n",
    "    def setup(self, config):\n",
    "        with FileLock(os.path.expanduser(\"~/.tune.lock\")):\n",
    "            self.train_stories, self.test_stories = read_data(\n",
    "                config[\"finish_fast\"])\n",
    "        model = self.build_model()\n",
    "        rmsprop = RMSprop(\n",
    "            lr=self.config.get(\"lr\", 1e-3), rho=self.config.get(\"rho\", 0.9))\n",
    "        model.compile(\n",
    "            optimizer=rmsprop,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "        self.model = model\n",
    "        \n",
    "    @mlflow_mixin\n",
    "    def step(self):\n",
    "        # train\n",
    "        mlflow.tensorflow.autolog()\n",
    "        self.model.fit(\n",
    "            [self.inputs_train, self.queries_train],\n",
    "            self.answers_train,\n",
    "            batch_size=self.config.get(\"batch_size\", 32),\n",
    "            epochs=self.config.get(\"epochs\", 1),\n",
    "            validation_data=([self.inputs_test, self.queries_test],\n",
    "                             self.answers_test),\n",
    "            verbose=0)\n",
    "        _, accuracy = self.model.evaluate(\n",
    "            [self.inputs_train, self.queries_train],\n",
    "            self.answers_train,\n",
    "            verbose=0)\n",
    "        return {\"mean_accuracy\": accuracy}\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        file_path = checkpoint_dir + \"/model\"\n",
    "        self.model.save(file_path)\n",
    "        return file_path\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        # See https://stackoverflow.com/a/42763323\n",
    "        del self.model\n",
    "        self.model = load_model(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6e6b4",
   "metadata": {},
   "source": [
    "## initialize a Ray cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c79082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Hyperplane: selecting worker node pool\n",
      "best pool spec {'pool_env_var': 'DASK_POOL_16_16', 'allocatable_cores': 15.0, 'allocatable_ram': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 15:02:31,381\tINFO services.py:1270 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-12-08 15:02:31,385\tWARNING services.py:1748 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=8.60gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for worker ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26...\n",
      "Waiting for worker ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be...\n"
     ]
    }
   ],
   "source": [
    "from hyperplane.ray_common import initialize_ray_cluster, stop_ray_cluster, find_ray_workers\n",
    "num_workers = 2\n",
    "cpu_core_per_worker = 15\n",
    "ram_gb_per_worker = 12 #110 GB allocatible for 16_128 nodes, 12 for 16_16 nodes, 27 for 32_32 nodes\n",
    "ray_cluster = initialize_ray_cluster(num_workers, cpu_core_per_worker, ram_gb_per_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6e71664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "pbt = PopulationBasedTraining(\n",
    "    perturbation_interval=2,\n",
    "    hyperparam_mutations={\n",
    "        \"dropout\": lambda: np.random.uniform(0, 1),\n",
    "        \"lr\": lambda: 10**np.random.randint(-10, 0),\n",
    "        \"rho\": lambda: np.random.uniform(0, 1)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e02f9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'pbt_babi_memnn'\n",
    "mlflow.set_tracking_uri(os.environ.get('DATABASE_URL_NO_PARAMS')[:-12]) ## this one \n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f8ad8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 15:07:49 (running for 00:00:00.17)<br>Memory usage on this node: 2.4/31.4 GiB<br>PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Resources requested: 0/37 CPUs, 0/0 GPUs, 0.0/32.38 GiB heap, 0.0/15.0 GiB objects<br>Result logdir: /root/ray_results/pbt_babi_memnn<br>Number of trials: 2/2 (2 PENDING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:50.528779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:51.107925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MemNNModel pid=73, ip=10.1.168.3)\u001b[0m Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
      "   73728/11745123 [..............................] - ETA: 13s\n",
      "  385024/11745123 [..............................] - ETA: 4s \n",
      " 5611520/11745123 [=============>................] - ETA: 0s\n",
      "11747328/11745123 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:54.598060: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:54.598522: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:54.598555: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:54.598597: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:54.598884: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:54.599566: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021/12/08 15:07:55 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '76da4bfc3a0348d2bcb9cf20aeb54563', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:55.214277: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:55.214320: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:55.214397: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:55.262928: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:55.263416: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200135000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MemNNModel pid=72, ip=10.1.167.6)\u001b[0m Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
      "    8192/11745123 [..............................] - ETA: 37s\n",
      "  278528/11745123 [..............................] - ETA: 5s \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:57.043080: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:57.043129: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:57.060707: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:57.067952: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:57.078221: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:57.086345: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.trace.json.gz\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:57.093000: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:57.093121: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.memory_profile.json.gz\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:07:57.093602: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57Dumped tool data for xplane.pb to /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.xplane.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m Dumped tool data for overview_page.pb to /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.overview_page.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m Dumped tool data for input_pipeline.pb to /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.input_pipeline.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m Dumped tool data for tensorflow_stats.pb to /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.tensorflow_stats.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m Dumped tool data for kernel_stats.pb to /tmp/tmpp0u_vntr/train/plugins/profile/2021_12_08_15_07_57/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.kernel_stats.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4792320/11745123 [===========>..................] - ETA: 0s\n",
      "11747328/11745123 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:58.648163: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:58.648699: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:58.648734: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:58.648763: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:58.649017: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:58.649835: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 15:07:58 (running for 00:00:09.62)<br>Memory usage on this node: 2.4/31.4 GiB<br>PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Resources requested: 2.0/37 CPUs, 0/0 GPUs, 0.0/32.38 GiB heap, 0.0/15.0 GiB objects<br>Result logdir: /root/ray_results/pbt_babi_memnn<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021/12/08 15:07:59 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '70941480a7c54cf1bb98ed663d74218a', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:59.489430: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:59.489491: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:59.489664: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:59.543847: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:07:59.544386: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200200000 Hz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 15:07:59 (running for 00:00:10.63)<br>Memory usage on this node: 2.4/31.4 GiB<br>PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Resources requested: 2.0/37 CPUs, 0/0 GPUs, 0.0/32.38 GiB heap, 0.0/15.0 GiB objects<br>Result logdir: /root/ray_results/pbt_babi_memnn<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:00.009744: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:01.570623: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:01.570672: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:01.596288: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:01.619962: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:01.648155: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:01.656368: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.trace.json.gz\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:01.710075: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:01.710272: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.memory_profile.json.gz\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:01.711198: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01Dumped tool data for xplane.pb to /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.xplane.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m Dumped tool data for overview_page.pb to /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.overview_page.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m Dumped tool data for input_pipeline.pb to /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.input_pipeline.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m Dumped tool data for tensorflow_stats.pb to /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.tensorflow_stats.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m Dumped tool data for kernel_stats.pb to /tmp/tmp7byfgk8t/train/plugins/profile/2021_12_08_15_08_01/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.kernel_stats.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial MemNNModel_9b515_00000 reported mean_accuracy=0.22 with parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'pbt_babi_memnn', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-m288j5y2'}}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021/12/08 15:08:03 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '95e460991eed4863b4b20f534ac1e845', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.415144: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.415177: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.415215: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.540455: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.540494: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.578940: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.586123: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.596493: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.604584: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.trace.json.gz\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.610391: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.610501: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.memory_profile.json.gz\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m 2021-12-08 15:08:03.611058: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03Dumped tool data for xplane.pb to /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.xplane.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m Dumped tool data for overview_page.pb to /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.overview_page.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m Dumped tool data for input_pipeline.pb to /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.input_pipeline.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m Dumped tool data for tensorflow_stats.pb to /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.tensorflow_stats.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m Dumped tool data for kernel_stats.pb to /tmp/tmpkvsfuu_l/train/plugins/profile/2021_12_08_15_08_03/ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be.kernel_stats.pb\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:05.119690: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 15:08:05 (running for 00:00:16.05)<br>Memory usage on this node: 2.4/31.4 GiB<br>PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Resources requested: 2.0/37 CPUs, 0/0 GPUs, 0.0/32.38 GiB heap, 0.0/15.0 GiB objects<br>Current best trial: 9b515_00000 with mean_accuracy=0.21875 and parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'pbt_babi_memnn', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-m288j5y2'}}<br>Result logdir: /root/ray_results/pbt_babi_memnn<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "\u001b[2m\u001b[36m(pid=73, ip=10.1.168.3)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial MemNNModel_9b515_00001 reported mean_accuracy=0.22 with parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'pbt_babi_memnn', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-m288j5y2'}}.\n",
      "Trial MemNNModel_9b515_00000 reported mean_accuracy=0.23 with parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'pbt_babi_memnn', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-m288j5y2'}}. This trial completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 15:08:11 (running for 00:00:21.93)<br>Memory usage on this node: 2.4/31.4 GiB<br>PopulationBasedTraining: 1 checkpoints, 0 perturbs<br>Resources requested: 1.0/37 CPUs, 0/0 GPUs, 0.0/32.38 GiB heap, 0.0/15.0 GiB objects<br>Current best trial: 9b515_00000 with mean_accuracy=0.234375 and parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'pbt_babi_memnn', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-m288j5y2'}}<br>Result logdir: /root/ray_results/pbt_babi_memnn<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021/12/08 15:08:14 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '505ff502b39148f99da5fdcfbe3f8f1a', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.653185: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.653224: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.653279: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.796667: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.796719: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.837495: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.845348: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.856050: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.864348: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.trace.json.gz\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.870517: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.870656: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.memory_profile.json.gz\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m 2021-12-08 15:08:14.871547: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14Dumped tool data for xplane.pb to /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.xplane.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m Dumped tool data for overview_page.pb to /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.overview_page.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m Dumped tool data for input_pipeline.pb to /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.input_pipeline.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m Dumped tool data for tensorflow_stats.pb to /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.tensorflow_stats.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m Dumped tool data for kernel_stats.pb to /tmp/tmp1j8p_e9w/train/plugins/profile/2021_12_08_15_08_14/ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26.kernel_stats.pb\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 15:08:16 (running for 00:00:26.94)<br>Memory usage on this node: 2.4/31.4 GiB<br>PopulationBasedTraining: 1 checkpoints, 0 perturbs<br>Resources requested: 1.0/37 CPUs, 0/0 GPUs, 0.0/32.38 GiB heap, 0.0/15.0 GiB objects<br>Current best trial: 9b515_00000 with mean_accuracy=0.234375 and parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'pbt_babi_memnn', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-m288j5y2'}}<br>Result logdir: /root/ray_results/pbt_babi_memnn<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "\u001b[2m\u001b[36m(pid=72, ip=10.1.167.6)\u001b[0m WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial MemNNModel_9b515_00001 reported mean_accuracy=0.28 with parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'pbt_babi_memnn', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-m288j5y2'}}. This trial completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 15:08:20 (running for 00:00:31.61)<br>Memory usage on this node: 2.4/31.4 GiB<br>PopulationBasedTraining: 1 checkpoints, 0 perturbs<br>Resources requested: 0/37 CPUs, 0/0 GPUs, 0.0/32.38 GiB heap, 0.0/15.0 GiB objects<br>Current best trial: 9b515_00001 with mean_accuracy=0.28125 and parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'pbt_babi_memnn', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-m288j5y2'}}<br>Result logdir: /root/ray_results/pbt_babi_memnn<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc          </th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>MemNNModel_9b515_00000</td><td>TERMINATED</td><td>10.1.168.3:73</td><td style=\"text-align: right;\">0.234375</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         14.3511</td></tr>\n",
       "<tr><td>MemNNModel_9b515_00001</td><td>TERMINATED</td><td>10.1.167.6:72</td><td style=\"text-align: right;\">0.28125 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         16.1567</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 15:08:21,074\tINFO tune.py:630 -- Total run time: 32.30 seconds (31.61 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tune.run(\n",
    "        MemNNModel,\n",
    "        name=\"pbt_babi_memnn\",\n",
    "        scheduler=pbt,\n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        stop={\"training_iteration\": 2},\n",
    "        num_samples=2,\n",
    "        config={\n",
    "            \"finish_fast\": True,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 1,\n",
    "            \"dropout\": 0.3,\n",
    "            \"lr\": 0.01,\n",
    "            \"rho\": 0.9,\n",
    "            \"mlflow\": {\n",
    "                \"experiment_name\": experiment_name,\n",
    "                \"tracking_uri\": mlflow.get_tracking_uri()\n",
    "            }\n",
    "        },\n",
    "        verbose = 2,\n",
    "        # sync_config=tune.SyncConfig(\n",
    "        # sync_to_driver=False,\n",
    "        # upload_dir=\"gs://shakdemo-hyperplane/results/ray_tf_nl/\"\n",
    "        # upload_dir=\"s3://d2v-tmp/demo/ray\"\n",
    "        # )\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddb66512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'pbt_babi_memnn', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-m288j5y2'}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", results.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f5f34a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting ray-worker-77eb2548-bc5f-43bd-9c1a-4ff59fb03b26\n",
      "Deleting ray-worker-e3fc8c5b-84c0-41b9-ba23-149d722384be\n"
     ]
    }
   ],
   "source": [
    "stop_ray_cluster(ray_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868142c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
